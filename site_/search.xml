<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[从语音到文本的end-to-end翻译]]></title>
      <url>/%E8%AE%BA%E6%96%87notes/2019/05/27/proof-of-concept-end2end/</url>
      <content type="text"><![CDATA[[LISTEN AND TRANSLATE: A PROOF OF CONCEPT FOR END-TO-END SPEECH-TO-TEXT TRANSLATION] 2016  End-to-end speech translation. Def. : the direct translation of an audio signal without intermediate transcription steps.现在的语音翻译系统整合使用两个主要模型：source language speech recognition（ASR）和 source-to-target text （MT）。在这些方法里，源语言的text transcript（sequence或者graph）用于生成目标语言的text hypothesis。当时的“encoder-decoder”结构是一种把输入信号的序列投射到连续的低维空间里并且从而生成输出序列。如果不考虑从源语音序列直接翻译到目标词序列或者从平行phone-word序列发现双语语料库，那么从源语音信号到目标语言文本的直接翻译的唯一尝试是Duong的源语音发音和它们的文本翻译之间的校准，以及Anastasopoulos提出用IBM Model 2和dynamic time warping(DTW)来校准源语音和目标文本，但当时Duong及另一论文都没有提出完整的end-to-end翻译系统。这篇论文首次尝试建立一个end-to-end speech-to-text translation system，并且在学习或者解码过程中不使用源语言文本。减少对源语言转录的需求可以改变语言翻译时的数据收集方式，从收集speech transcription变成双语者看到目标语言文本的表达然后直接以源语言说话。Model比较两个end-to-end模型：标准机器翻译和语音翻译（都使用了attention-based encoder-decoder神经网络）。输入序列(), 多层双向的LSTM的encoder生成输出序列h=(). decoder初始状态, 是encoder最后一个状态。decoder的下一个状态是是LSTM的新状态，是输出。在训练时，是目标序列reference中的前一个标记；评估时，是前一个time-step的预测标记。E是目标嵌入矩阵。与LSTM的输出级联的attention vector this vector is mapped to a vector of the same size as the target vocabulary,to compute a probability distribution over all target words每个time-step greedy decoder（每次只选最可能的那一个词）选取概率最大时的。在文本输入时用了注意力机制（attention mechanism）为了能在语音输入时让attention model记住并且不重复翻译同一部分信号，用了卷积注意力模型（convolutional attention），用convolutional filter来记录前一个time-step的attention weights。这个对于输入语音序列特别长的时候特别有用。Experiments他们生成了一种有语音合成的语音翻译语料库，使用小型平行语料库French-English BTEC corpus。因为这两个语种有相似的语序（SVO-SVO主动宾）。语音文本转换模型比机器翻译模型表现差一点。Conclusion这篇论文提出的end-to-end语音翻译模型的结果建立在小型法-英合成语料库，首次证明了在不使用源语言的情况下进行端到端语音到文本翻译的可能性。]]></content>
      <categories>
        
          <category> 论文notes </category>
        
      </categories>
      <tags>
        
          <tag> speech translation </tag>
        
          <tag> NMT </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Windows的Linux子系统：bash打开application的图形界面]]></title>
      <url>/%E8%AE%B0%E5%BD%95/2019/05/16/Windows-Subsystem-bash-open/</url>
      <content type="text"><![CDATA[因为想要用linux环境下的LaTex，所以需要texmaker的图形界面。在没有配置前的ubuntu子系统只有terminal。然后整理了一下步骤：      安装VcXsrv: https://sourceforge.net/projects/vcxsrv/files/latest/download        运行XLaunch    bash中运行texmaker就可以在VcXsrv中打开界面码字了    然后关于不是很需要的Unity桌面：        bash输入：     sudo apt-get install ubuntu-desktop unity compizconfig-settings-manager        打开ccsm     ccsm        然后再VcXsrv Server勾选需要的模块，close。     echo "export DISPLAY=:0.0" &gt;&gt; ~/.bashrc sudo sed -i 's$&lt;listen&gt;.*&lt;/listen&gt;$&lt;listen&gt;tcp:host=localhost,port=0&lt;/listen&gt;$' /etc/dbus-1/session.conf        启动Unity桌面     compiz      ]]></content>
      <categories>
        
          <category> 记录 </category>
        
      </categories>
      <tags>
        
          <tag> linux </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[关于light weight和dynamic convolution]]></title>
      <url>/%E8%AE%BA%E6%96%87notes/2019/05/16/pla-with-lightweight-dynconv/</url>
      <content type="text"><![CDATA[      [PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTGIONS]    这篇论文关注通过lightweight convolutions和dynamica convolutions来解决MT中较长序列问题，降低网络参数，提升训练速度。                  Transformer                        Transformer可以用来解决自然语言不定长问题，跟CNN类似，设定输入的最大长度，用Padding填充不够的部分，使模型输入定长。Transformer为了保留输入句子单词之间的相对位置信息，用位置函数进行位置编码，在输入端输入位置信息。Transformer的第一层是Multi-head self attention。                    Self Attention                        Self Attention 会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，因为这个特性self attention可以解决NLP句子中长距离依赖特征问题。相比较，RNN需要通过隐层节点往后传，CNN需要通过增加网络深度来捕获远距离特征。                    Lightweight Convolution                    Dynamic Convolution                    Experiment                    Results                    Conclustion            ]]></content>
      <categories>
        
          <category> 论文notes </category>
        
      </categories>
      <tags>
        
          <tag> NMT </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Launches Site]]></title>
      <url>/2019/05/16/first-time-to-post/</url>
      <content type="text"><![CDATA[something still wrong?]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
